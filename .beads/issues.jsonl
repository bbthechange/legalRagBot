{"id":"legalRag-2sg","title":"Unit 3: Multi-Source Document Refactor","status":"closed","priority":2,"issue_type":"feature","owner":"bbthechange@gmail.com","created_at":"2026-02-23T01:31:24.204282-08:00","created_by":"Brian Butler","updated_at":"2026-02-23T01:38:17.324092-08:00","closed_at":"2026-02-23T01:38:17.324092-08:00","close_reason":"All 6 steps implemented, 99/99 tests pass"}
{"id":"legalRag-6pu","title":"Unit 2: Citation Grounding \u0026 Draft Framing","description":"Implement per tmp/tasks/unit-2-citation-grounding-and-draft-framing.md. Add source IDs to retrieval context, update all 3 prompt strategies for citations + confidence, structured pipeline output with sources + draft framing, update CLI output, update evaluation module. LABELS: model:sonnet","notes":"Completed Unit 2: Citation Grounding \u0026 Draft Framing. All 5 deliverables implemented cleanly. Deliverable 1 (retrieval.py) was already done from a prior commit — the format_retrieval_results() function already used Source [id] format. Deliverables 2-5 were implemented as specified: updated all 3 prompt strategies with citation instructions (basic, structured, few-shot), updated rag_pipeline.py to parse LLM output via parse_json_response_or_raw and return sources + draft framing instead of retrieved_clauses, updated main.py display, and updated evaluation.py to stringify dict analysis for the judge prompt. One minor issue: the few-shot system prompt didn't initially contain 'source' or 'cite', causing one test to fail — fixed by adding a single brief sentence to FEW_SHOT_SYSTEM_PROMPT. All 118 tests pass. MockProvider updated with sources_used and confidence fields. New test file test_citation_grounding.py has 17 tests covering all spec requirements. No unexpected difficulties.","status":"closed","priority":2,"issue_type":"task","owner":"bbthechange@gmail.com","created_at":"2026-02-23T01:18:18.861356-08:00","created_by":"Brian Butler","updated_at":"2026-02-23T02:04:57.741105-08:00","closed_at":"2026-02-23T02:04:57.741105-08:00","close_reason":"Closed","labels":["model:sonnet"]}
{"id":"legalRag-afi","title":"Unit 5: Data Ingestion Pipeline + CUAD","description":"Implement per tmp/tasks/unit-5-data-ingestion-cuad.md. Build ingestion framework (BaseIngestor), ClausesJsonIngestor wrapper, CuadIngestor for HuggingFace CUAD dataset, orchestrator script, batch embedding with rate awareness, verification script. LABELS: model:sonnet","notes":"Debrief: Implemented Unit 5 in full. Created src/ingest/ package with BaseIngestor (ABC with ingest pipeline), ClausesJsonIngestor, CuadIngestor, and ingest_all.py orchestrator. Added scripts/verify_ingestion.py. Modified src/embeddings.py to add batch_size parameter to get_embeddings() with batched API calls. Added datasets\u003e=2.14.0 to requirements.txt. Wrote 5 test files (44 tests total): test_ingest_base.py, test_ingest_clauses_json.py, test_ingest_cuad.py, test_ingest_all.py, test_batch_embeddings.py. All 162 tests pass (162 including pre-existing tests; test_api.py excluded due to pre-existing fastapi not installed in env). Implementation followed task spec exactly. No difficulties — codebase was well-structured from prior units and schemas.py was already in place. The test for test_ingest_clauses_json.py::test_clauses_json_real_file passes cleanly verifying the real data/clauses.json has exactly 15 docs. CuadIngestor tests use only mock data as required (no HuggingFace download). One pre-existing issue: test_api.py fails on collection due to fastapi not being pip-installed in the environment — this is not related to Unit 5 work.","status":"closed","priority":2,"issue_type":"task","owner":"bbthechange@gmail.com","created_at":"2026-02-23T01:18:19.90799-08:00","created_by":"Brian Butler","updated_at":"2026-02-23T02:12:10.398874-08:00","closed_at":"2026-02-23T02:12:10.398874-08:00","close_reason":"Closed","labels":["model:sonnet"],"dependencies":[{"issue_id":"legalRag-afi","depends_on_id":"legalRag-cjg","type":"blocks","created_at":"2026-02-23T01:18:58.253072-08:00","created_by":"Brian Butler"}]}
{"id":"legalRag-cjg","title":"Unit 4: FastAPI API Layer","description":"Implement per tmp/tasks/unit-4-fastapi-layer.md. FastAPI app with /analyze, /search, /health endpoints. API key auth via X-API-Key header. Pydantic request/response models. Request logging middleware. Tests with TestClient. LABELS: model:sonnet","notes":"Debrief: Implemented Unit 4 FastAPI layer cleanly. Created src/api_models.py with Pydantic v2 request/response models, src/api.py with the three endpoints (/health, /analyze, /search), request logging middleware, and X-API-Key auth. Tests (tests/test_api.py, 38 tests) all pass. One difficulty: FastAPI's TestClient re-raises server exceptions by default rather than returning a 500 response — fixed the server_error_returns_500 test by using raise_server_exceptions=False on a dedicated TestClient instance. Also had to install fastapi/uvicorn/httpx into the project venv (the default system python3.14 had a different environment). The task spec code was followed closely; no significant deviations. Follow-up suggestion: consider adding an exception handler in api.py (app.exception_handler) to return consistent JSON 500 error bodies rather than FastAPI's default HTML/text error.","status":"closed","priority":2,"issue_type":"task","owner":"bbthechange@gmail.com","created_at":"2026-02-23T01:18:19.518704-08:00","created_by":"Brian Butler","updated_at":"2026-02-23T02:08:19.018156-08:00","closed_at":"2026-02-23T02:08:19.018156-08:00","close_reason":"Closed","labels":["model:sonnet"],"dependencies":[{"issue_id":"legalRag-cjg","depends_on_id":"legalRag-6pu","type":"blocks","created_at":"2026-02-23T01:18:58.023728-08:00","created_by":"Brian Butler"}]}
