"""
Evaluation Module â€” Measuring RAG System Quality

Evaluates retrieval and generation quality separately so we can
pinpoint which component to improve. Uses ground truth test cases
and an LLM-as-Judge pattern for generation scoring.
"""

import json
import logging
import time

from src.embeddings import load_clause_database
from src.retrieval import search_similar_clauses
from src.rag_pipeline import analyze_clause

logger = logging.getLogger(__name__)

# Ground truth test cases for evaluation.
# Each includes expected retrieval IDs, risk level, and required issues.
TEST_CASES = [
    {
        "name": "Aggressive IP Assignment",
        "clause": """All inventions, works, and ideas conceived by Employee at any time
        during employment shall become the sole property of the Company,
        regardless of whether created during work hours or using Company resources.""",
        "expected_similar_ids": ["emp-004", "emp-003"],
        "expected_risk_level": "high",
        "must_identify": [
            "overbroad scope",
            "no carve-out for personal work",
        ],
    },
    {
        "name": "Reasonable Confidentiality",
        "clause": """Both parties agree to keep confidential all proprietary information
        shared under this agreement. Confidential information does not include
        information that is publicly available or independently developed.
        This obligation survives for three years after termination.""",
        "expected_similar_ids": ["nda-001", "nda-004"],
        "expected_risk_level": "low",
        "must_identify": [
            "standard exclusions present",
            "reasonable time limitation",
        ],
    },
    {
        "name": "One-Sided Termination",
        "clause": """The Provider may cancel this agreement at any time with 7 days notice.
        The Client may only terminate for material breach, subject to a 60-day
        cure period. Early termination by Client requires payment of all
        remaining fees for the full contract term.""",
        "expected_similar_ids": ["svc-004", "svc-003"],
        "expected_risk_level": "high",
        "must_identify": [
            "asymmetric termination rights",
            "early termination penalty",
        ],
    },
    {
        "name": "Weak Data Protection",
        "clause": """The vendor will make reasonable efforts to safeguard client data.
        In the event of a security incident, vendor will notify client
        in a timely manner. Vendor may use anonymized client data for
        product improvement purposes.""",
        "expected_similar_ids": ["svc-006", "svc-005"],
        "expected_risk_level": "medium",
        "must_identify": [
            "vague protection standard",
            "no specific breach notification timeline",
        ],
    },
]


def evaluate_retrieval(db: dict, top_k: int = 3) -> dict:
    """
    Measure retrieval quality using standard IR metrics.

    Metrics:
    - Recall@k: Fraction of expected clauses found in top-k results
    - Mean Reciprocal Rank (MRR): Ranking quality of correct results
    """
    logger.info("Evaluating retrieval with top_k=%d across %d test cases", top_k, len(TEST_CASES))
    results = {"test_cases": [], "recall_at_k": 0.0, "mrr": 0.0}
    total_recall = 0.0
    total_rr = 0.0

    for test in TEST_CASES:
        retrieved = search_similar_clauses(test["clause"], db, top_k=top_k)
        retrieved_ids = [r["clause"]["id"] for r in retrieved]

        expected = set(test["expected_similar_ids"])
        found = expected.intersection(set(retrieved_ids))
        recall = len(found) / len(expected) if expected else 0
        rr = 0.0
        for rank, rid in enumerate(retrieved_ids, 1):
            if rid in expected:
                rr = 1.0 / rank
                break

        total_recall += recall
        total_rr += rr

        results["test_cases"].append({
            "name": test["name"],
            "expected": test["expected_similar_ids"],
            "retrieved": retrieved_ids,
            "recall": recall,
            "reciprocal_rank": rr,
            "scores": [f"{r['score']:.3f}" for r in retrieved],
        })

    n = len(TEST_CASES)
    results["recall_at_k"] = total_recall / n
    results["mrr"] = total_rr / n
    return results


JUDGE_PROMPT = """You are evaluating the quality of a legal contract analysis.

The analysis was generated by an AI system. Score it on these criteria:

1. RISK_ACCURACY (1-5): Does the identified risk level match the expected level?
   Expected risk level: {expected_risk}

2. ISSUE_COVERAGE (1-5): Does the analysis identify these key issues?
   Required issues: {must_identify}

3. ACTIONABILITY (1-5): Are the suggested revisions specific and practical?

4. GROUNDING (1-5): Does the analysis reference the retrieved similar clauses
   rather than making unsupported claims?

Return ONLY a JSON object:
{{
    "risk_accuracy": <1-5>,
    "issue_coverage": <1-5>,
    "actionability": <1-5>,
    "grounding": <1-5>,
    "total": <sum of above>,
    "notes": "Brief explanation of scores"
}}

ANALYSIS TO EVALUATE:
{analysis}"""


def evaluate_generation(db: dict, strategy: str = "few_shot") -> dict:
    """
    Use the LLM-as-Judge pattern to score generated analyses.

    Scores on 4 dimensions: risk accuracy, issue coverage,
    actionability, and grounding. A stronger model (e.g. gpt-4o)
    could be used as the judge to reduce self-evaluation bias.
    """
    logger.info("Evaluating generation with strategy=%s across %d test cases", strategy, len(TEST_CASES))
    results = {"test_cases": [], "strategy": strategy, "avg_scores": {}}
    all_scores = {"risk_accuracy": [], "issue_coverage": [],
                  "actionability": [], "grounding": [], "total": []}

    for test in TEST_CASES:
        pipeline_result = analyze_clause(
            test["clause"], db, strategy=strategy
        )

        judge_messages = [
            {"role": "system", "content": "You are an expert legal AI evaluator. Return only valid JSON."},
            {"role": "user", "content": JUDGE_PROMPT.format(
                expected_risk=test["expected_risk_level"],
                must_identify=", ".join(test["must_identify"]),
                analysis=pipeline_result["analysis"],
            )},
        ]

        judge_response = db["provider"].chat(judge_messages, temperature=0.0)

        try:
            scores = json.loads(judge_response)
        except (json.JSONDecodeError, TypeError):
            scores = {"risk_accuracy": 0, "issue_coverage": 0,
                      "actionability": 0, "grounding": 0, "total": 0,
                      "notes": "Failed to parse judge response"}

        for key in all_scores:
            all_scores[key].append(scores.get(key, 0))

        results["test_cases"].append({
            "name": test["name"],
            "expected_risk": test["expected_risk_level"],
            "scores": scores,
        })

    # Averages
    for key in all_scores:
        vals = all_scores[key]
        results["avg_scores"][key] = sum(vals) / len(vals) if vals else 0

    return results


def compare_strategies(db: dict) -> dict:
    """
    Run evaluation across all prompt strategies and compare results.
    Provides an empirical basis for selecting the best prompt approach.
    """
    comparison = {}

    for strategy_name in ["basic", "structured", "few_shot"]:
        print(f"\nEvaluating strategy: {strategy_name}...")
        start = time.time()
        gen_results = evaluate_generation(db, strategy=strategy_name)
        elapsed = time.time() - start

        comparison[strategy_name] = {
            "avg_scores": gen_results["avg_scores"],
            "time_seconds": round(elapsed, 1),
            "details": gen_results["test_cases"],
        }

    return comparison


def print_retrieval_results(results: dict):
    """Print retrieval evaluation in a readable format."""
    print("\n" + "=" * 60)
    print("RETRIEVAL EVALUATION RESULTS")
    print("=" * 60)

    for tc in results["test_cases"]:
        status = "PASS" if tc["recall"] == 1.0 else "PARTIAL" if tc["recall"] > 0 else "FAIL"
        print(f"\n[{status}] {tc['name']}")
        print(f"  Expected:  {tc['expected']}")
        print(f"  Retrieved: {tc['retrieved']} (scores: {tc['scores']})")
        print(f"  Recall: {tc['recall']:.0%} | RR: {tc['reciprocal_rank']:.2f}")

    print(f"\n--- OVERALL ---")
    print(f"Mean Recall@k: {results['recall_at_k']:.0%}")
    print(f"Mean Reciprocal Rank: {results['mrr']:.2f}")


def print_comparison_results(comparison: dict):
    """Print strategy comparison in a readable format."""
    print("\n" + "=" * 60)
    print("STRATEGY COMPARISON RESULTS")
    print("=" * 60)

    # Summary table
    print(f"\n{'Strategy':<15} {'Risk Acc':>10} {'Coverage':>10} {'Action':>10} {'Ground':>10} {'TOTAL':>10} {'Time':>8}")
    print("-" * 75)

    for name, data in comparison.items():
        avg = data["avg_scores"]
        print(
            f"{name:<15} "
            f"{avg.get('risk_accuracy', 0):>10.1f} "
            f"{avg.get('issue_coverage', 0):>10.1f} "
            f"{avg.get('actionability', 0):>10.1f} "
            f"{avg.get('grounding', 0):>10.1f} "
            f"{avg.get('total', 0):>10.1f} "
            f"{data['time_seconds']:>7.1f}s"
        )

    # Winner
    best = max(comparison.items(), key=lambda x: x[1]["avg_scores"].get("total", 0))
    print(f"\nBest strategy: {best[0]} (total score: {best[1]['avg_scores'].get('total', 0):.1f}/20)")
