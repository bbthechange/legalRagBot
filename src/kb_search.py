"""
Unified knowledge base search pipeline.

Routes queries, retrieves from the appropriate collections,
and generates cited answers.
"""

import logging

from src.query_router import route_query
from src.retrieval import search_similar_clauses, format_retrieval_results
from src.generation import generate_analysis, build_knowledge_base_qa_prompt
from src.output_parser import parse_json_response_or_raw

logger = logging.getLogger(__name__)


def search_knowledge_base(
    query: str,
    db: dict,
    top_k: int = 5,
    use_router: bool = True,
) -> dict:
    """
    Full knowledge base search pipeline.

    1. Route the query to determine search strategy
    2. Retrieve relevant documents with appropriate filters
    3. Generate a cited answer

    Returns:
        {
            "answer": parsed answer dict or raw text,
            "routing": routing decision dict,
            "sources": list of source info dicts,
            "review_status": "pending_review",
            "disclaimer": str,
        }
    """
    # Step 1: Route
    if use_router:
        routing = route_query(query, db["provider"])
        logger.info(f"Query routed: type={routing['query_type']}, "
                    f"strategy={routing['search_strategy']}, "
                    f"filters={routing.get('filters', {})}")
    else:
        routing = {
            "query_type": "general_legal",
            "filters": {},
            "search_strategy": "semantic",
            "rewritten_query": None,
        }

    # Step 2: Retrieve
    search_query = routing.get("rewritten_query") or query
    filters = routing.get("filters", {})

    # For hybrid, use filters; for semantic, no filters; for filtered, use filters only
    search_filters = filters if routing["search_strategy"] in ("filtered", "hybrid") else None

    results = search_similar_clauses(
        search_query, db, top_k=top_k, filters=search_filters
    )

    if not results:
        return {
            "answer": {
                "answer": "I don't have enough information in the knowledge base to answer this question.",
                "sources_used": [],
                "confidence": "low",
                "caveats": ["No relevant documents found for this query."],
                "related_queries": [],
            },
            "routing": routing,
            "sources": [],
            "review_status": "pending_review",
            "disclaimer": "DRAFT — Requires Attorney Review.",
        }

    # Step 3: Generate answer
    context = format_retrieval_results(results)
    messages = build_knowledge_base_qa_prompt(query, context)
    raw_answer = generate_analysis(messages, db["provider"], temperature=0.2)
    parsed_answer = parse_json_response_or_raw(raw_answer)

    # Build source trail
    sources = []
    for r in results:
        clause = r["clause"]
        sources.append({
            "id": clause.get("id", "unknown"),
            "title": clause.get("title", ""),
            "score": round(r["score"], 4),
            "source": clause.get("source", ""),
            "doc_type": clause.get("doc_type", ""),
        })

    return {
        "answer": parsed_answer,
        "routing": routing,
        "sources": sources,
        "review_status": "pending_review",
        "disclaimer": (
            "DRAFT RESPONSE — Requires Attorney Review. "
            "This answer was generated by an AI system and citations "
            "should be verified against primary sources."
        ),
    }
